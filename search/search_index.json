{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Spark SQL DataFrame Symmetric Difference Engine","text":"<p>The concept of symmetric difference in mathematics defines the set of elements that do not appear in either of the sets. It is leveraged within the RDBMS community as a strategy for identifying differences across two tables. <code>diffit</code> extends this capability to Apache Spark SQL DataFrames.</p> <p>Diffit was created in response to challenges around validating massive data sets that use big data file formats. However, Diffit is also flexible enough to be used within your unit test framework.</p>"},{"location":"containerisation/","title":"Scaling out Diffit","text":"<p>Diffit can be scaled out to perform symmetric difference analysis across massive data sets. This is a natural extension as Diffit is built onto of the Apache Spark compute framework</p>"},{"location":"containerisation/#diffit-container-image","title":"Diffit Container Image","text":"<p>Note</p> <p>Diffit image container to be provided via Docker Hub shortly.</p>"},{"location":"containerisation/#manual-build-of-the-diffit-container-image","title":"Manual Build of the Diffit Container Image","text":"<p>The Diffit image container is built on top of this Spark base image that runs over YARN on Pseudo Distributed Hadoop. The goal here is to establish a pathway into a larger Spark cluster.</p> <p>The container image build process is started with: Starting the Diffit container image build process<pre><code>make diffit-image-build\n</code></pre></p> <p>On successful completion, you can search for the newly created container image with: Container image search<pre><code>make image-search\n</code></pre></p>"},{"location":"containerisation/#running-jobs-on-the-diffit-container-image","title":"Running jobs on the Diffit Container Image","text":"<p>Refer to the Diffit <code>Makefile</code> for the targets used in this section.</p> <p>Note</p> <p>All Makester Docker subsystem targets can be listed with: <pre><code>make docker-help`\n</code></pre></p> <p>Here is how the <code>diffit row csv</code> example would be run in the container image: Running diffit compute on container image<pre><code>CMD=\"row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\" make container-run\n</code></pre></p> <p>This is the actual <code>docker</code> command: Diffit container run make target<pre><code>make container-run\n</code></pre></p> <pre><code>/usr/bin/docker run --rm -d\\\n--platform linux/amd64\\\n--publish 8032:8032\\\n--publish 7077:7077\\\n--publish 8080:8080\\\n--publish 8088:8088\\\n--publish 8042:8042\\\n--publish 18080:18080\\\n--env CORE_SITE__HADOOP_TMP_DIR=/tmp\\\n--env HDFS_SITE__DFS_REPLICATION=1\\\n--env YARN_SITE__YARN_NODEMANAGER_RESOURCE_DETECT_HARDWARE_CAPABILITIES=true\\\n--env YARN_SITE__YARN_LOG_AGGREGATION_ENABLE=true\\\n--env OUTPUT_PATH=file:///tmp/data/out\\\n--env NUM_EXECUTORS=6\\\n--env EXECUTOR_CORES=2\\\n--env EXECUTOR_MEMORY=768m\\\n--volume $HOME/diffit/docker/files/python:/data\\\n--volume $HOME/diffit/docker/files/schema:/tmp/schema\\\n--volume $HOME/diffit/docker/files/data:/tmp/data\\\n--hostname diffit-spark\\\n--name diffit-spark\\\ndiffit:0.1.2a4-1\\\nrow csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\n</code></pre> <p>Note</p> <p>Container image execution is ephemeral. The <code>--output</code> switch ensures that the results persist on the file system via the mount provided by <code>--volume $HOME/diffit/docker/files/data:/tmp/data</code></p> <p>Progress logs of the container execution can be monitored as follows: diffit on Spark pseudo-distributed logging<pre><code>make container-logs\n</code></pre></p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Diffit can be used to validate PySpark transformation and aggregation logic at the Spark SQL DataFrame level. The <code>diffit</code> package can be installed by adding the following to your project's <code>setup.cfg</code>: setup.cfg as an extras require.<pre><code>[options.extras_require]\ndev =\ndiffit @ git+https://github.com/loum/diffit.git@&lt;release_version&gt;\n</code></pre></p> <p>Note</p> <p>See Diffit releases for latest version.</p> <p>Editable installation can be achieved as follows:</p> pip editable install.<pre><code>pip install -e .[dev]\n</code></pre>"},{"location":"getting-started/#unit-testing","title":"Unit Testing","text":"<p>Here is a trivial use case that checks that two Spark SQL DataFrame are the same:</p> A trivial Diffit check.<pre><code>def test_symmetric_difference_check_ok(spark: SparkSession, dummy: DataFrame):\n    \"\"\"Symmetric difference check: no error.\"\"\"\n# Given a Dummy Spark DataFrame\n# dummy\n# when I compare it against itself at the DataFrame level\ndiffs: DataFrame = diffit.reporter.row_level(left=dummy, right=dummy)\n# then the Diffit integration check should not detect an error\nmsg = \"Symmetric difference SHOULD NOT BE detected.\"\nassert not [list(r) for r in diffs.collect()], msg\n</code></pre> <p>The <code>diffit</code> call of interest is <code>diffit.reporter.row_level</code>. Here, a symmetric difference is performed against the <code>left</code> Spark SQL DataFrame named <code>dummy</code> onto itself.</p> <p>For brievity, the <code>assert</code> simply validates that the <code>diff</code> variable does not contain content. If <code>diffit</code> produces output, then this indicates that there is a problem and the test will fail. As the <code>diff</code> variable is just a Spark SQL DataFrame itself, you can manipulate <code>diff</code> content to further investigate the error.</p>"},{"location":"reference/diffit/reporter/","title":"reporter","text":"<p>Diffit <code>diffit.reporter</code>.</p> <p>top</p>"},{"location":"reference/diffit/reporter/#diffit.reporter.altered_rows","title":"<code>altered_rows(diff, column_key, range_filter=None)</code>","text":"<p>Return a DataFrame of altered rows relative to diff.</p> <p>Works on a Differ output DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame of rows that different.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def altered_rows(\ndiff: DataFrame,\ncolumn_key: Text,\nrange_filter: Optional[RangeFilter] = None,\n) -&gt; DataFrame:\n\"\"\"Return a DataFrame of altered rows relative to *diff*.\n    Works on a Differ output DataFrame.\n    Returns:\n        DataFrame of rows that different.\n    \"\"\"\nif range_filter is not None and range_filter.column in diff.columns:\ncondition: Optional[Column] = range_filter.range_filter_clause(diff.schema)\nif condition is not None:\ndiff = diff.filter(condition)\nreturn diff.filter(\ndiff[column_key].isin(\ngrouped_rows(diff, column_key, group_count=2)\n.rdd.flatMap(lambda x: x)\n.collect()\n)\n)\n</code></pre>"},{"location":"reference/diffit/reporter/#diffit.reporter.altered_rows_column_diffs","title":"<code>altered_rows_column_diffs(diff, column_key, key_val)</code>","text":"<p>Helper function that creates a new, reduced DataFrame from the Differ output diff and captures only the columns that are different. Column value differences are reported as a Python dictionary.</p> <p>column_key provides unique constraint behaviour while its value key_val filters a targeted row set.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def altered_rows_column_diffs(\ndiff: DataFrame, column_key: Text, key_val: Union[int, Text]\n) -&gt; Iterable[Dict]:\n\"\"\"Helper function that creates a new, reduced DataFrame from the Differ output *diff*\n    and captures only the columns that are different. Column value differences\n    are reported as a Python dictionary.\n    *column_key* provides unique constraint behaviour while its value *key_val* filters\n    a targeted row set.\n    \"\"\"\nif key_val is not None:\ndiff = diff.filter(F.col(column_key) == key_val)\ncol_diffs = altered_rows(diff, column_key)\nleft = col_diffs.filter(F.col(\"diffit_ref\") == \"left\").drop(F.col(\"diffit_ref\"))\nright = col_diffs.filter(F.col(\"diffit_ref\") == \"right\").drop(F.col(\"diffit_ref\"))\nreturn diffit.column_level_diff(left, right)\n</code></pre>"},{"location":"reference/diffit/reporter/#diffit.reporter.distinct_rows","title":"<code>distinct_rows(diff, column_key, diffit_ref='left')</code>","text":"<p>Return a DataFrame of unique rows relative to diff.</p> <p>Works on a Differ output DataFrame.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def distinct_rows(\ndiff: DataFrame, column_key: Text, diffit_ref: Text = \"left\"\n) -&gt; DataFrame:\n\"\"\"Return a DataFrame of unique rows relative to *diff*.\n    Works on a Differ output DataFrame.\n    \"\"\"\nreturn diff.filter(\ndiff[column_key].isin(\ngrouped_rows(diff, column_key).rdd.flatMap(lambda x: x).collect()\n)\n).filter(diff.diffit_ref == diffit_ref)\n</code></pre>"},{"location":"reference/diffit/reporter/#diffit.reporter.get_columns","title":"<code>get_columns(columns_to_add, columns_to_drop)</code>","text":"<p>Determine the columns to include in the symantic check.</p> <p>Parameters:</p> Name Type Description Default <code>columns_to_add</code> <code>List[Text]</code> <p>List of columns add to the diffit check.</p> required <code>columns_to_drop</code> <code>List[Text]</code> <p>List of columns that can be omitted from the diffit check.</p> required <p>Returns:</p> Type Description <code>List[Text]</code> <p>A sorted list of columns produced after the list subtraction of <code>columns_to_drop</code> from <code>columns_to_add</code>.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def get_columns(\ncolumns_to_add: List[Text],\ncolumns_to_drop: List[Text],\n) -&gt; List[Text]:\n\"\"\"Determine the columns to include in the symantic check.\n    Parameters:\n        columns_to_add: List of columns add to the diffit check.\n        columns_to_drop: List of columns that can be omitted from the diffit check.\n    Returns:\n        A sorted list of columns produced after the list subtraction of `columns_to_drop` from\n            `columns_to_add`.\n    \"\"\"\nreturn sorted([x for x in columns_to_add if x not in columns_to_drop])\n</code></pre>"},{"location":"reference/diffit/reporter/#diffit.reporter.grouped_rows","title":"<code>grouped_rows(diff, column_key, group_count=1)</code>","text":"<p>Return a DataFrame of grouped rows from DataFrame diff where column column_key acts as the unique constraint.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def grouped_rows(diff: DataFrame, column_key: Text, group_count: int = 1) -&gt; DataFrame:\n\"\"\"Return a DataFrame of grouped rows from DataFrame *diff* where column *column_key*\n    acts as the unique constraint.\n    \"\"\"\nreturn (\ndiff.groupBy(column_key)\n.agg(F.count(column_key).alias(\"count\"))\n.filter(F.col(\"count\") == group_count)\n.select(column_key)\n)\n</code></pre>"},{"location":"reference/diffit/reporter/#diffit.reporter.row_level","title":"<code>row_level(left, right, columns_to_add=None, columns_to_drop=None, range_filter=None)</code>","text":"<p>Wrapper function to report on differences between left and right Spark SQL DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>DataFrame</code> <p>Source DataFrame orientation.</p> required <code>right</code> <code>DataFrame</code> <p>Source DataFrame orientation.</p> required <code>columns_to_add</code> <code>Optional[List[Text]]</code> <p>List of columns add to the diffit check.</p> <code>None</code> <code>columns_to_drop</code> <code>Optional[List[Text]]</code> <p>List of columns that can be omitted from the diffit check.</p> <code>None</code> <code>range_filter</code> <code>Optional[RangeFilter]</code> <p>Data structure that sets the thresholds for range filtering.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spark DataFrame of different rows between the source DataFrames under test.</p> Source code in <code>diffit/reporter/__init__.py</code> <pre><code>def row_level(\nleft: DataFrame,\nright: DataFrame,\ncolumns_to_add: Optional[List[Text]] = None,\ncolumns_to_drop: Optional[List[Text]] = None,\nrange_filter: Optional[RangeFilter] = None,\n) -&gt; DataFrame:\n\"\"\"Wrapper function to report on differences between *left* and *right*\n    Spark SQL DataFrames.\n    Parameters:\n        left: Source DataFrame orientation.\n        right: Source DataFrame orientation.\n        columns_to_add: List of columns add to the diffit check.\n        columns_to_drop: List of columns that can be omitted from the diffit check.\n        range_filter: Data structure that sets the thresholds for range filtering.\n    Returns:\n        Spark DataFrame of different rows between the source DataFrames under test.\n    \"\"\"\nlog.info(\"columns_to_add: %s\", columns_to_add)\nif columns_to_drop is None:\ncolumns_to_drop = []\nif not columns_to_add or columns_to_add is None:\ncolumns_to_add = left.columns\ncolumns: List[Text] = get_columns(columns_to_add, columns_to_drop)\nlog.info(\"Running row level difference check on columns: %s\", \",\".join(columns))\nleft = left.select(*columns)\nright = right.select(*columns)\nif range_filter is not None and range_filter.column in left.columns:\nfilter_clause: Optional[Column] = range_filter.range_filter_clause(left.schema)\nif filter_clause is not None:\nleft = left.filter(filter_clause)\nright = right.filter(filter_clause)\nlog.info(\"Starting diff report ...\")\nsymmetric = diffit.symmetric_level(left, right)\nreturn diffit.symmetric_filter(left, symmetric).union(\ndiffit.symmetric_filter(right, symmetric, orientation=\"right\")\n)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/","title":"spark","text":"<p>SparkSession as a data source.</p> <p>top</p>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.Mode","title":"<code>Mode</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Text</code>, <code>Enum</code></p> <p>Spark DataFrame write modes.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>@dataclass\nclass Mode(Text, Enum):\n\"\"\"Spark DataFrame write modes.\"\"\"\nAPPEND: ClassVar[Text] = \"append\"\nERROR: ClassVar[Text] = \"error\"\nIGNORE: ClassVar[Text] = \"ignore\"\nOVERWRITE: ClassVar[Text] = \"overwrite\"\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.aws_spark_conf","title":"<code>aws_spark_conf(conf=None)</code>","text":"<p>AWS authentication config.</p> <p>Parameters:</p> Name Type Description Default <code>conf</code> <code>Optional[SparkConf]</code> <p>Optional SparkConf to be extended. Otherwise, creates a new SparkConf.</p> <code>None</code> <p>Returns:</p> Type Description <code>SparkConf</code> <p>A SparkConf instance with AWS auth support.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def aws_spark_conf(conf: Optional[SparkConf] = None) -&gt; SparkConf:\n\"\"\"AWS authentication config.\n    Parameters:\n        conf: Optional SparkConf to be extended. Otherwise, creates a new SparkConf.\n    Returns:\n        A SparkConf instance with AWS auth support.\n    \"\"\"\nif conf is None:\nconf = SparkConf()\nconf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\")\nconf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.ap-southeast-2.amazonaws.com\")\nconf.set(\"spark.hadoop.fs.s3a.aws.experimental.input.fadvise\", \"random\")\naws_path = os.path.join(pathlib.Path.home(), \".aws\", \"credentials\")\nif os.path.exists(aws_path):\nconf.set(\n\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n\"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\",\n)\nwith open(aws_path, encoding=\"utf-8\") as _fh:\naws_config = ConfigParser()\naws_config.read_file(_fh)\nconf.set(\n\"spark.hadoop.fs.s3a.access.key\",\naws_config.get(\"default\", \"aws_access_key_id\"),\n)\nconf.set(\n\"spark.hadoop.fs.s3a.secret.key\",\naws_config.get(\"default\", \"aws_secret_access_key\"),\n)\nconf.set(\n\"spark.hadoop.fs.s3a.session.token\",\naws_config.get(\"default\", \"aws_session_token\"),\n)\nelse:\nkms_key_arn = os.environ.get(\"KMS_KEY_ARN\")\nif kms_key_arn:\nconf.set(\n\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n\"org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider\",\n)\nconf.set(\"spark.hadoop.fs.s3a.server-side-encryption-algorithm\", \"SSE-KMS\")\nconf.set(\"spark.hadoop.fs.s3a.server-side-encryption.key\", kms_key_arn)\nreturn conf\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.csv_reader","title":"<code>csv_reader(spark, schema, csv_path, delimiter=',', header=True)</code>","text":"<p>Spark CSV reader.</p> <p>Setting such as delimiter and header can be adjusted during the read.</p> <p>Returns a DataFrame representation of the CSV.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def csv_reader(\nspark: SparkSession,\nschema: StructType,\ncsv_path: Text,\ndelimiter: Text = \",\",\nheader: bool = True,\n) -&gt; DataFrame:\n\"\"\"Spark CSV reader.\n    Setting such as *delimiter* and *header* can be adjusted during the read.\n    Returns a DataFrame representation of the CSV.\n    \"\"\"\nreturn (\nspark.read.schema(schema)\n.option(\"delimiter\", delimiter)\n.option(\"ignoreTrailingWhiteSpace\", \"true\")\n.option(\"ignoreLeadingWhiteSpace\", \"true\")\n.option(\"header\", header)\n.option(\"emptyValue\", None)\n.option(\"quote\", '\"')\n.csv(csv_path)\n)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.json_reader","title":"<code>json_reader(spark, source_path, schema, multiline=False)</code>","text":"<p>Read in JSON files from source_path directory.</p> <p>Here, we leave nothing to chance. So you must provide a schema.</p> <p>Returns a Spark SQL DataFrame.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def json_reader(\nspark: SparkSession, source_path: Text, schema: StructType, multiline: bool = False\n) -&gt; DataFrame:\n\"\"\"Read in JSON files from *source_path* directory.\n    Here, we leave nothing to chance. So you must provide a *schema*.\n    Returns a Spark SQL DataFrame.\n    \"\"\"\nreturn (\nspark.read.schema(schema)\n.option(\"multiline\", multiline)\n.option(\"mode\", \"FAILFAST\")\n.json(source_path)\n)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.json_writer","title":"<code>json_writer(source, outpath, mode=Mode.OVERWRITE)</code>","text":"<p>Write out Spark DataFrame <code>source</code> to <code>outpath</code> directory as JSON. The write mode is defined by <code>mode</code>.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def json_writer(source: DataFrame, outpath: Text, mode: Mode = Mode.OVERWRITE) -&gt; None:\n\"\"\"Write out Spark DataFrame `source` to `outpath` directory as JSON.\n    The write mode is defined by `mode`.\n    \"\"\"\nsource.write.mode(mode).json(outpath)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.parquet_reader","title":"<code>parquet_reader(spark, source_path)</code>","text":"<p>Read in Spark Parquet files from source_path directory.</p> <p>Returns a Spark SQL DataFrame.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def parquet_reader(spark: SparkSession, source_path: Text) -&gt; DataFrame:\n\"\"\"Read in Spark Parquet files from *source_path* directory.\n    Returns a Spark SQL DataFrame.\n    \"\"\"\nlog.info('Reading Parquet data from \"%s\"', source_path)\nreturn spark.read.parquet(source_path)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.parquet_writer","title":"<code>parquet_writer(dataframe, outpath, mode=Mode.OVERWRITE)</code>","text":"<p>Write out Spark DataFrame <code>dataframe</code> to <code>outpath</code> directory as Spark Parquet.</p> <p>The write mode is defined by <code>mode</code>.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def parquet_writer(\ndataframe: DataFrame, outpath: Text, mode: Mode = Mode.OVERWRITE\n) -&gt; None:\n\"\"\"Write out Spark DataFrame `dataframe` to `outpath` directory as Spark Parquet.\n    The write mode is defined by `mode`.\n    \"\"\"\nlog.info(\"Writing Parquet to location: %s\", outpath)\ndataframe.write.mode(mode).parquet(outpath)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.sanitise_columns","title":"<code>sanitise_columns(source, problematic_chars=',;{}()=')</code>","text":"<p>As the diffit engine produces a parquet output, we may need to remove special characters from the source headers that do not align with the parquet conventions. The column sanitise step will:</p> <ul> <li>convert to lower case</li> <li>replace spaces with under-score</li> <li>strip out the problematic_char set</li> </ul> <p>Returns:</p> Type Description <code>DataFrame</code> <p>New DataFrame with adjusted columns.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def sanitise_columns(\nsource: DataFrame, problematic_chars: Text = \",;{}()=\"\n) -&gt; DataFrame:\n\"\"\"As the diffit engine produces a parquet output, we may need\n    to remove special characters from the source headers that do not\n    align with the parquet conventions. The column sanitise step will:\n    - convert to lower case\n    - replace spaces with under-score\n    - strip out the *problematic_char* set\n    Returns:\n        New DataFrame with adjusted columns.\n    \"\"\"\nnew_columns = []\nfor column in source.columns:\ncolumn = column.lower()\ncolumn = column.replace(\" \", \"_\")\nfor _char in problematic_chars:\ncolumn = column.replace(_char, \"\")\nnew_columns.append(column)\nreturn source.toDF(*new_columns)\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.spark_conf","title":"<code>spark_conf(app_name, conf=None)</code>","text":"<p>Set up the SparkContext with appropriate config for test.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>Text</code> <p>Application name to provide to the SparkSession.</p> required <code>conf</code> <code>Optional[SparkConf]</code> <p>Optional SparkConf to be extended. Otherwise, creates a new SparkConf.</p> <code>None</code> <p>Returns:</p> Type Description <code>SparkConf</code> <p>SparkConf construct.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def spark_conf(app_name: Text, conf: Optional[SparkConf] = None) -&gt; SparkConf:\n\"\"\"Set up the SparkContext with appropriate config for test.\n    Parameters:\n        app_name: Application name to provide to the SparkSession.\n        conf: Optional SparkConf to be extended. Otherwise, creates a new SparkConf.\n    Returns:\n        SparkConf construct.\n    \"\"\"\nif conf is None:\nconf = SparkConf()\n# Common settings.\nconf.setAppName(app_name)\nconf.set(\"spark.ui.port\", \"4050\")\nconf.set(\"spark.logConf\", \"true\")\nconf.set(\"spark.debug.maxToStringFields\", \"100\")\nconf.set(\"spark.sql.session.timeZone\", \"UTC\")\nconf.set(\"spark.sql.jsonGenerator.ignoreNullFields\", \"false\")\nreturn conf\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.spark_session","title":"<code>spark_session(app_name=diffit.__app_name__, conf=None)</code>","text":"<p>SparkSession.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def spark_session(\napp_name: Text = diffit.__app_name__, conf: Optional[SparkConf] = None\n) -&gt; SparkSession:\n\"\"\"SparkSession.\"\"\"\nreturn SparkSession.builder.config(\nconf=spark_conf(app_name=app_name, conf=conf)\n).getOrCreate()\n</code></pre>"},{"location":"reference/diffit/datastore/spark/#diffit.datastore.spark.split_dir","title":"<code>split_dir(directory_path, directory_token)</code>","text":"<p>Helper function to strip leading directory parts from `directory until directory_token* is matched.</p> <p>Returns:</p> Type Description <code>Optional[Text]</code> <p>The remaining parts of <code>directory_path</code> as a string.</p> Source code in <code>diffit/datastore/spark.py</code> <pre><code>def split_dir(directory_path: Text, directory_token: Text) -&gt; Optional[Text]:\n\"\"\"Helper function to strip leading directory parts from `directory*\n    until *directory_token* is matched.\n    Returns:\n        The remaining parts of `directory_path` as a string.\n    \"\"\"\ndirectory_parts = deque(directory_path.split(os.sep))\nnew_path = None\nwhile directory_parts:\nif directory_parts.popleft() == directory_token:\nnew_path = os.path.join(*directory_parts)\nbreak\nreturn new_path\n</code></pre>"},{"location":"utilities/","title":"Usage","text":"<p><code>diffit</code> provides a command line utility that you can use to invoke the Apache Spark-based symmetric differential engine.</p> <pre><code>venv/bin/diffit --help\n</code></pre> diffit usage message.<pre><code> Usage: diffit [OPTIONS] COMMAND [ARGS]...\n\nDiff-it Data Diff tool.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --quiet                        Disable logs to screen (to log level \"ERROR\").         \u2502\n\u2502 --driver-memory  -M      TEXT  Set Spark driver memory. [default: 1g]                 \u2502\n\u2502 --help                         Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 analyse        List rows that are unique to the nominated source DataFrame.           \u2502\n\u2502 columns        Display the columns that are different.                                \u2502\n\u2502 convert        Convert CSV to Apache Parquet.                                         \u2502\n\u2502 row            Filter out identical rows from the left and right data sources.        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/analyse/","title":"Usage","text":"<p><code>diffit analyse</code> works over the <code>diffit</code> engine output that is created with diffit row.</p> <p><code>diffit analyse</code> supports the concept of a unique constraint key to target differences at the row level.</p> <pre><code>venv/bin/diffit analyse --help\n</code></pre> diffit analyse usage message.<pre><code> Usage: diffit analyse [OPTIONS] COMMAND [ARGS]...\n\nList rows that are unique to the nominated source DataFrame.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 distinct              Spark DataFrame list rows source data.         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/analyse/altered/","title":"altered","text":"<p><code>diffit analyse altered</code> provides a way to report on <code>diffit</code> engine extract rows that appear in both target data sources and flagged as being different.</p> <pre><code>venv/bin/diffit analyse altered --help\n</code></pre> diffit analyse altered usage message.<pre><code> Usage: diffit analyse altered [OPTIONS] PARQUET_PATH\n\nSpark DataFrame list rows source data.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    parquet_path      TEXT  Path to Spark Parquet: input [required]                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --key           -k      TEXT     Analysis column to act as a unique constraint [default: None] [required] \u2502\n\u2502    --range-column  -r      TEXT     Column to target for range filter                                        \u2502\n\u2502    --lower         -L      INTEGER  Range filter lower bound (inclusive)                                     \u2502\n\u2502    --upper         -U      INTEGER  Range filter upper bound (inclusive)                                     \u2502\n\u2502    --force-range   -F               Force (cast) string-based range column filter                            \u2502\n\u2502    --descending    -D               Change output ordering to descending                                     \u2502\n\u2502    --counts-only   -C               Only output counts                                                       \u2502\n\u2502    --hits          -H      INTEGER  Rows to display [default: 20]                                            \u2502\n\u2502    --help                           Show this message and exit.                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/analyse/altered/#example","title":"Example","text":"<p>Note</p> <p>The following examples source sample diffit engine output data that can be found at <code>docker/files/parquet/analysis</code>.</p> diffit analyse altered command with schema column col01 as the unique constraint.<pre><code>venv/bin/diffit analyse altered --key col01 docker/files/parquet/analysis\n</code></pre> Result.<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n|8    |col02_val08|col03_val08|left      |\n|8    |col02_val08|col03_valYY|right     |\n+-----+-----------+-----------+----------+\n</code></pre> <p>To reverse the order of the output (based on <code>key</code> column <code>col01</code>):</p> diffit analyse altered command reverse ordering.<pre><code>venv/bin/diffit analyse altered --key col01 --descending docker/files/parquet/analysis\n</code></pre> Result.<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|8    |col02_val08|col03_val08|left      |\n|8    |col02_val08|col03_valYY|right     |\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n+-----+-----------+-----------+----------+\n</code></pre> <p>To only report on the counts:</p> diffit analyse altered command counts only output.<pre><code>venv/bin/diffit analyse altered --key col01 --counts-only docker/files/parquet/analysis\n</code></pre> <p>Result.<pre><code>Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n...\n2023-02-16 13:18:44 logga [INFO]: Reading Parquet data from \"docker/files/parquet/analysis\"\n2023-02-16 13:18:48 logga [INFO]: Altered rows analysis count: 2\n</code></pre> ```</p>"},{"location":"utilities/diffit/analyse/distinct/","title":"distinct","text":"<p><code>diffit analyse distinct</code> provides a way to report on rows from a <code>diffit</code> engine extract that only appear in either one of the <code>left</code> or <code>right</code> target data sources.</p> <p><pre><code>venv/bin/diffit analyse distinct --help\n</code></pre> diffit analyse distinct usage message.<pre><code> Usage: diffit analyse distinct [OPTIONS] PARQUET_PATH\n\nSpark DataFrame list rows source data.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    parquet_path      TEXT  Path to Spark Parquet: input [required]                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    --orientation   -O      [left|right]  Limit analysis orientation to either \"left\" or \"right\"          \u2502\n\u2502 *  --key           -k      TEXT          Analysis column to act as a unique constraint [default: None]   \u2502\n\u2502                                          [required]                                                      \u2502\n\u2502    --descending    -D                    Change output ordering to descending                            \u2502\n\u2502    --counts-only   -C                    Only output counts                                              \u2502\n\u2502    --hits          -H      INTEGER       Rows to display [default: 20]                                   \u2502\n\u2502    --help                                Show this message and exit.                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"utilities/diffit/analyse/distinct/#example-analyse-rows-unique-to-each-spark-dataframe","title":"Example: Analyse Rows Unique to Each Spark DataFrame","text":"<p>Note</p> <p>The following examples source sample <code>diffit</code> engine output data that can be found at <code>docker/files/parquet/analysis</code>.</p> The key setting, col01, acts as the GROUP BY predicate.<pre><code>venv/bin/diffit analyse distinct --key col01 docker/files/parquet/analysis\n</code></pre> Combined diffit analyse distinct output.<pre><code>### Analysing distinct rows from \"left\" source DataFrame\n+-----+-----+-----+----------+\n|col01|col02|col03|diffit_ref|\n+-----+-----+-----+----------+\n+-----+-----+-----+----------+\n\n### Analysing distinct rows from \"right\" source DataFrame\n+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|9    |col02_val09|col03_val09|right     |\n+-----+-----------+-----------+----------+\n</code></pre> <p>A Diffit extract can be limited with the <code>--orientation</code> switch. For example, to only show distinct Diffit extract records from the <code>right</code> data source:</p> Analysing distinct rows from right source Spark DataFrame.<pre><code>venv/bin/diffit analyse distinct --orientation right --key col01 docker/files/parquet/analysis\n</code></pre> Result.<pre><code>### Analysing distinct rows from \"right\" source DataFrame\n+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|9    |col02_val09|col03_val09|right     |\n+-----+-----------+-----------+----------+\n</code></pre>"},{"location":"utilities/diffit/columns/","title":"Usage","text":"<p>Display the column names where the Spark DataFrame column/value pair show differences.</p> <p><code>diffit</code> engine analysis for altered columns will display the entire row. For wide row, this could become unweildy to manage. <code>diffit columns</code> allows you to only list on the key/value pairing column names that allow a targeted analysis.</p> <p>Output is displayed as a JSON construct.</p> <pre><code>venv/bin/diffit columns --help\n</code></pre> diffit columns usage message.<pre><code> Usage: diffit columns [OPTIONS] COMMAND [ARGS]...\n\nDisplay the columns that are different\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 diff         Analyse altered column name differences.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/columns/diff/","title":"diff","text":"<p>Report on Spark DataFrame column/value pair differences.</p> <p>Diffit extracts can be large, based on the number of exceptions detected. <code>diffit columns</code> allows you to report on a specific key/value pairing for targeted analysis.</p> <p>Output is displayed as a JSON construct.</p> <pre><code>venv/bin/diffit columns diff --help\n</code></pre> diffit columns diff usage message.<pre><code> Usage: diffit columns diff [OPTIONS] PARQUET_PATH\n\nAnalyse altered column name differences.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    parquet_path      TEXT  Path to Spark Parquet: input [required]                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --key    -k      TEXT  Analysis column to act as a unique constraint [default: None] [required]           \u2502\n\u2502 *  --value  -V      TEXT  Unique constraint column value to filter against [required]                        \u2502\n\u2502    --help                 Show this message and exit.                                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/columns/diff/#example","title":"Example","text":"<p>Note</p> <p>The following examples source sample diffit engine output data that can be found at <code>docker/files/parquet/analysis</code>.</p> <p>The <code>diffit</code> engine extract at <code>docker/files/parquet/analysis</code> features:</p> <ul> <li>A schema column <code>col01</code> acting as the unique constraint.</li> <li>A key column value of <code>2</code> as a filter.</li> </ul> diffit columns diff filter for key:value pair col01:2<pre><code>venv/bin/diffit columns diff --key col01 --value 2 docker/files/parquet/analysis\n</code></pre> Result.<pre><code>Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n...\n2023-02-16 14:20:33 logga [INFO]: Reading Parquet data from \"docker/files/parquet/analysis\"\n2023-02-16 14:20:38 logga [INFO]: key|val:\n[\n{\n\"col02\": \"col02_val02\"\n}\n]\n</code></pre> <p>Note</p> <p>The value listed under the key/value result is a taken from the <code>left</code> orinented data source. </p>"},{"location":"utilities/diffit/convert/","title":"Usage","text":"<p>Convert as data source to Spark Parquet to with a given compression (default <code>snappy</code>).</p> <p>Note</p> <p>Only CSV format is supported at this time.</p> <pre><code>venv/bin/diffit convert --help\n</code></pre> diffit convert CSV to Spark Parquet usage message.<pre><code> Usage: diffit convert [OPTIONS] COMMAND [ARGS]...\n\nConvert CSV to Apache Parquet.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 csv           Convert CSV to Apache Parquet.               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/convert/csv/","title":"csv","text":"<p>Convert as CSV data source with schema file to Spark Parquet to with a given compression (default <code>snappy</code>)</p> <pre><code>venv/bin/diffit convert csv --help\n</code></pre> diffit convert csv usage message.<pre><code>i Usage: diffit convert csv [OPTIONS] PARQUET_PATH\n\nConvert CSV to Apache Parquet.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    parquet_path      TEXT  Path to Spark Parquet: output [required]                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --json-schema       -J      TEXT                                   Path to CSV schema in JSON format  \u2502\n\u2502                                                                       [required]                         \u2502\n\u2502 *  --csv-data          -C      TEXT                                   Path to CSV data source [required] \u2502\n\u2502    --compression-type  -Z      [brotli|gzip|lz4|lzo|none|snappy|unco  Compression type [default: snappy] \u2502\n\u2502                                mpressed|zstd]                                                            \u2502\n\u2502    --num-partitions    -N      INTEGER                                Number of partitions [default: 8]  \u2502\n\u2502    --help                                                             Show this message and exit.        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/convert/csv/#example","title":"Example","text":"<p>Convert sample CSV into Apache Parquet. The following CSV sample schema is re-used from previous examples.</p> <pre><code>venv/bin/diffit convert csv --csv-separator \";\" --json-schema /tmp/Dummy.json --csv-data docker/files/data/left /tmp/converted\n</code></pre>"},{"location":"utilities/diffit/row/","title":"Usage","text":"<p><code>diffit row</code> reporter acts on two data sources with the same schema. These are denoted as <code>left</code> and <code>right</code>. Differences are reported at the row level. Both CSV and Spark Parquet data sources are supported. </p> <p>Another way to think about the <code>diffit row</code> reporter is that identical rows from the <code>left</code> and <code>right</code> data sources are suppressed from the output.</p> <p><code>diffit row</code> will produce a Diffit extract that is a Spark DataFrame in Spark Parquet format. The Diffit extract can then be further analysed using other <code>diffit</code> subcommands see <code>diffit analyse</code> or with any other tooling that supports parquet.</p> <p>A key characteristic of the Diffit extract is that it features a new column <code>diffit_ref</code>. This denotes the source reference that has caused the row level exception. Typically, this value will be either <code>left</code> or <code>right</code>.</p> <pre><code>venv/bin/diffit row --help\n</code></pre> diffit row usage message.<pre><code> Usage: diffit row [OPTIONS] COMMAND [ARGS]...\n\nFilter out identical rows from the left and right data sources.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 csv            Spark DataFrame row-level diff from CSV source data.              \u2502\n\u2502 parquet        Spark DataFrame row-level diff from Spark Parquet source data.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/row/csv/","title":"csv","text":"<p>CSV source data files will require a schema definition. This needs to be provided as a JSON construct.</p> <pre><code>venv/bin/diffit row csv --help\n</code></pre> diffit row csv usage message.<pre><code> Usage: diffit row csv [OPTIONS]\nSpark DataFrame row-level diff from CSV source data.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    --add            -a      TEXT     Add column to the diffit engine                       \u2502\n\u2502    --drop           -d      TEXT     Drop column from diffit engine                        \u2502\n\u2502    --range-column   -r      TEXT     Column to target for range filter                     \u2502\n\u2502    --lower          -L      INTEGER  Range filter lower bound (inclusive)                  \u2502\n\u2502    --upper          -U      INTEGER  Range filter upper bound (inclusive)                  \u2502\n\u2502    --force-range    -F               Force (cast) string-based range column filter         \u2502\n\u2502    --csv-separator  -s      TEXT     CSV separator [default: ,]                            \u2502\n\u2502    --csv-header     -E               CSV contains header                                   \u2502\n\u2502 *  --json-schema    -J      TEXT     Path to CSV schema in JSON format [required]          \u2502\n\u2502 *  --left           -l      TEXT     Path to left data source [required]                   \u2502\n\u2502 *  --right          -r      TEXT     Path to right data source [required]                  \u2502\n\u2502    --parquet-path           TEXT     Path to Spark Parquet: output                         \u2502\n\u2502    --help                            Show this message and exit.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/row/csv/#example-csv-data-sources","title":"Example: CSV Data Sources","text":""},{"location":"utilities/diffit/row/csv/#sample-csv-schema","title":"Sample CSV schema","text":"<p>Save the following sample JSON schema definition to <code>/tmp/Dummy.json</code>: Example CSV JSON Schema.<pre><code>{\n\"type\": \"struct\",\n    \"fields\": [\n{\n\"name\" : \"col01\",\n            \"type\" : \"integer\",\n            \"nullable\": true,\n            \"metadata\": {}\n},\n        {\n\"name\": \"col02\",\n            \"type\": \"string\",\n            \"nullable\": true,\n            \"metadata\": {}\n},\n        {\n\"name\": \"col03\",\n            \"type\": \"string\",\n            \"nullable\": true,\n            \"metadata\": {}\n}\n]\n}\n</code></pre></p> <p>Next, run the CSV row comparitor: diffit row csv command.<pre><code>venv/bin/diffit row csv --csv-separator ';' --json-schema /tmp/Dummy.json --left docker/files/data/left --right docker/files/data/right\n</code></pre></p> diffit row csv example output.<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|8    |col02_val08|col03_val08|left      |\n|9    |col02_val09|col03_val09|right     |\n|2    |col02_valXX|col03_val02|right     |\n|8    |col02_val08|col03_valYY|right     |\n+-----+-----------+-----------+----------+\n</code></pre>"},{"location":"utilities/diffit/row/csv/#report-on-subset-of-spark-dataframe-columns","title":"Report on Subset of Spark DataFrame Columns","text":"<p><code>diffit</code> can be run on a subset of DataFrame columns. This can limit the symmetric difference checker to a reduced number of colums for more targeted, efficient processing.</p> <p>To remove one or more unwanted Spark DataFrame columns use the <code>drop</code> switch.</p>"},{"location":"utilities/diffit/row/csv/#example-csv-data-sources-with-column-filtering","title":"Example: CSV Data Sources with Column Filtering","text":"<p>The following example will drop <code>col02</code> from the test sample:</p> diffit row csv dropping a column from the symmetric differential engine.<pre><code>venv/bin/diffit row csv --drop col02 --csv-separator ';' --json-schema /tmp/Dummy.json --left docker/files/data/left --right docker/files/data/right\n</code></pre> Column filtering result.<pre><code>+-----+-----------+----------+\n|col01|col03      |diffit_ref|\n+-----+-----------+----------+\n|8    |col03_val08|left      |\n|8    |col03_valYY|right     |\n|9    |col03_val09|right     |\n+-----+-----------+----------+\n</code></pre> <p>Multiple columns can be added to the <code>drop</code> switch separated by spaces. For example: <pre><code>... --drop col01 --drop col02 ... --drop &lt;col_n&gt; </code></pre></p> Dropping multiple columns from symmetric differential engine.<pre><code>venv/bin/diffit row csv --drop col02 --drop col03 --csv-separator ';' --json-schema /tmp/Dummy.json --left docker/files/data/left --right docker/files/data/right\n</code></pre> Multiple column filtering result.<pre><code>+-----+----------+\n|col01|diffit_ref|\n+-----+----------+\n|9    |right     |\n+-----+----------+\n</code></pre>"},{"location":"utilities/diffit/row/csv/#column-value-range-filtering","title":"Column Value Range Filtering","text":"<p>Note</p> <p>Only <code>pyspark.sql.types.IntegerType</code> is currently supported.</p> <p>Value range filtering can be useful to limit <code>diffit</code> to a subset of rows from the original Spark SQL DataFrame.</p>"},{"location":"utilities/diffit/row/csv/#example-csv-data-sources-with-output-reduced-through-range-filtering","title":"Example: CSV Data Sources with Output Reduced through Range Filtering","text":"<p>This example limits the test data sources under <code>docker/files/data/left</code> by removing <code>col01</code> values <code>3</code> and above:</p> Column range filtering.<pre><code>venv/bin/diffit row csv --range-column col01 --lower 1 --upper 2 --csv-separator ';' --json-schema /tmp/Dummy.json --left docker/files/data/left --right docker/files/data/right\n</code></pre> Result.<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n+-----+-----------+-----------+----------\n</code></pre>"},{"location":"utilities/diffit/row/parquet/","title":"parquet","text":"<p>Take advantage of the nice features of Spark Parquet.</p> <pre><code>venv/bin/diffit row parquet --help\n</code></pre> diffir row parquet usage message.<pre><code> Usage: diffit row parquet [OPTIONS]\nSpark DataFrame row-level diff from Spark Parquet source data.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    --add           -a      TEXT     Add column to the diffit engine                        \u2502\n\u2502    --drop          -d      TEXT     Drop column from diffit engine                         \u2502\n\u2502    --range-column  -r      TEXT     Column to target for range filter                      \u2502\n\u2502    --lower         -L      INTEGER  Range filter lower bound (inclusive)                   \u2502\n\u2502    --upper         -U      INTEGER  Range filter upper bound (inclusive)                   \u2502\n\u2502    --force-range   -F               Force (cast) string-based range column filter          \u2502\n\u2502 *  --left          -l      TEXT     Path to left data source [required]                    \u2502\n\u2502 *  --right         -r      TEXT     Path to right data source [required]                   \u2502\n\u2502    --parquet-path          TEXT     Path to Spark Parquet: output                          \u2502\n\u2502    --help                           Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"utilities/diffit/row/parquet/#report-on-subset-of-spark-dataframe-columns","title":"Report on Subset of Spark DataFrame Columns","text":"<p><code>diffit</code> can be run on a subset of DataFrame columns. This can limit the symmetric difference checker to a reduced number of colums for more targeted, efficient processing.</p> <p>To remove one or more unwanted Spark DataFrame columns use the <code>drop</code> switch.</p>"},{"location":"utilities/diffit/row/parquet/#example-parquet-data-sources-with-column-filtering","title":"Example: Parquet Data Sources with Column Filtering","text":"<p>To drop <code>col02</code> from the local test sample:</p> diffit row Parquet dropping a column from the symmetric differential engine.<pre><code>venv/bin/diffit row parquet --drop col02 --left docker/files/parquet/left --right docker/files/parquet/right\n</code></pre> Result.<pre><code>+-----+-----------+----------+\n|col01|col03      |diffit_ref|\n+-----+-----------+----------+\n|8    |col03_val08|left      |\n|8    |col03_valYY|right     |\n|9    |col03_val09|right     |\n+-----+-----------+----------+\n</code></pre> <p>Multiple columns can be added to the <code>drop</code> switch separated by spaces. For example: <pre><code>... --drop col01 --drop col02 ... --drop &lt;col_n&gt; </code></pre></p> Dropping multiple columns from symmetric differential engine.<pre><code>venv/bin/diffit row parquet --drop col02 --drop col03 --left docker/files/parquet/left --right docker/files/parquet/right\n</code></pre> Result.<pre><code>+-----+----------+\n|col01|diffit_ref|\n+-----+----------+\n|9    |right     |\n+-----+----------+\n</code></pre> <p>The same result can be achieved by adding selected columns to the symmetric difference checker:</p> diffit row Parquet adding columns to the symmetric differential engine.<pre><code>venv/bin/diffit row parquet --add col01 --add col03 --left docker/files/parquet/left --right docker/files/parquet/right\n</code></pre> Result.<pre><code>+-----+-----------+----------+\n|col01|col03      |diffit_ref|\n+-----+-----------+----------+\n|8    |col03_val08|left      |\n|8    |col03_valYY|right     |\n|9    |col03_val09|right     |\n+-----+-----------+----------+\n</code></pre>"},{"location":"utilities/diffit/row/parquet/#column-value-range-filtering","title":"Column Value Range Filtering","text":"<p>Note</p> <p>Only <code>pyspark.sql.types.IntegerType</code> is currently supported.</p> <p>Column range filtering can be useful to limit <code>diffit</code> to a subset of the original Spark SQL DataFrame.</p>"},{"location":"utilities/diffit/row/parquet/#example-csv-data-sources-with-output-reduced-through-range-filtering","title":"Example: CSV Data Sources with Output Reduced through Range Filtering","text":"<p>To limit the test data sources under <code>docker/files/data/left</code> to remove <code>col01</code> values <code>3</code> and above as follows:</p> Column range filtering.<pre><code>venv/bin/diffit row parquet --range-column col01 --lower 1 --upper 2 --left docker/files/parquet/left --right docker/files/parquet/right\n</code></pre> Result.<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n+-----+-----------+-----------+----------\n</code></pre>"}]}