{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Spark SQL DataFrame Symmetric Difference Engine The concept of symmetric difference in mathematics defines the set of elements that do not appear in either of the sets. It is leveraged within the RDBMS community as a strategy for identifying differences across two tables. diffit extends this capability to Apache Spark SQL DataFrames. Diffit was created in response to challenges around validating massive data sets that use big data file formats. However, Diffit is also flexible enough to be used within your unit test framework.","title":"Home"},{"location":"#a-spark-sql-dataframe-symmetric-difference-engine","text":"The concept of symmetric difference in mathematics defines the set of elements that do not appear in either of the sets. It is leveraged within the RDBMS community as a strategy for identifying differences across two tables. diffit extends this capability to Apache Spark SQL DataFrames. Diffit was created in response to challenges around validating massive data sets that use big data file formats. However, Diffit is also flexible enough to be used within your unit test framework.","title":"A Spark SQL DataFrame Symmetric Difference Engine"},{"location":"containerisation/","text":"Scaling out Diffit Diffit can be scaled out to perform symmetric difference analysis across massive data sets. This is a natural extension as Diffit is built onto of the Apache Spark compute framework Diffit Container Image Note Diffit image container to be provided via Docker Hub shortly. Manual Build of the Diffit Container Image The Diffit image container is built on top of this Spark base image that runs over YARN on Pseudo Distributed Hadoop. The goal here is to establish a pathway into a larger Spark cluster. The container image build process is started with: Starting the Diffit container image build process make diffit-image-build On successful completion, you can search for the newly created container image with: Container image search make image-search Running jobs on the Diffit Container Image Refer to the Diffit Makefile for the targets used in this section. Note All Makester Docker subsystem targets can be listed with: make docker-help ` Here is how the diffit row csv example would be run in the container image: Running diffit compute on container image CMD = \"row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\" make container-run This is the actual docker command: Diffit container run make target make container-run /usr/bin/docker run --rm -d \\ --platform linux/amd64 \\ --publish 8032 :8032 \\ --publish 7077 :7077 \\ --publish 8080 :8080 \\ --publish 8088 :8088 \\ --publish 8042 :8042 \\ --publish 18080 :18080 \\ --env CORE_SITE__HADOOP_TMP_DIR = /tmp \\ --env HDFS_SITE__DFS_REPLICATION = 1 \\ --env YARN_SITE__YARN_NODEMANAGER_RESOURCE_DETECT_HARDWARE_CAPABILITIES = true \\ --env YARN_SITE__YARN_LOG_AGGREGATION_ENABLE = true \\ --env OUTPUT_PATH = file:///tmp/data/out \\ --env NUM_EXECUTORS = 6 \\ --env EXECUTOR_CORES = 2 \\ --env EXECUTOR_MEMORY = 768m \\ --volume $HOME /diffit/docker/files/python:/data \\ --volume $HOME /diffit/docker/files/schema:/tmp/schema \\ --volume $HOME /diffit/docker/files/data:/tmp/data \\ --hostname diffit-spark \\ --name diffit-spark \\ diffit:0.1.2a4-1 \\ row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right Note Container image execution is ephemeral. The --output switch ensures that the results persist on the file system via the mount provided by --volume $HOME/diffit/docker/files/data:/tmp/data Progress logs of the container execution can be monitored as follows: diffit on Spark pseudo-distributed logging make container-logs","title":"Scaling out"},{"location":"containerisation/#scaling-out-diffit","text":"Diffit can be scaled out to perform symmetric difference analysis across massive data sets. This is a natural extension as Diffit is built onto of the Apache Spark compute framework","title":"Scaling out Diffit"},{"location":"containerisation/#diffit-container-image","text":"Note Diffit image container to be provided via Docker Hub shortly.","title":"Diffit Container Image"},{"location":"containerisation/#manual-build-of-the-diffit-container-image","text":"The Diffit image container is built on top of this Spark base image that runs over YARN on Pseudo Distributed Hadoop. The goal here is to establish a pathway into a larger Spark cluster. The container image build process is started with: Starting the Diffit container image build process make diffit-image-build On successful completion, you can search for the newly created container image with: Container image search make image-search","title":"Manual Build of the Diffit Container Image"},{"location":"containerisation/#running-jobs-on-the-diffit-container-image","text":"Refer to the Diffit Makefile for the targets used in this section. Note All Makester Docker subsystem targets can be listed with: make docker-help ` Here is how the diffit row csv example would be run in the container image: Running diffit compute on container image CMD = \"row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\" make container-run This is the actual docker command: Diffit container run make target make container-run /usr/bin/docker run --rm -d \\ --platform linux/amd64 \\ --publish 8032 :8032 \\ --publish 7077 :7077 \\ --publish 8080 :8080 \\ --publish 8088 :8088 \\ --publish 8042 :8042 \\ --publish 18080 :18080 \\ --env CORE_SITE__HADOOP_TMP_DIR = /tmp \\ --env HDFS_SITE__DFS_REPLICATION = 1 \\ --env YARN_SITE__YARN_NODEMANAGER_RESOURCE_DETECT_HARDWARE_CAPABILITIES = true \\ --env YARN_SITE__YARN_LOG_AGGREGATION_ENABLE = true \\ --env OUTPUT_PATH = file:///tmp/data/out \\ --env NUM_EXECUTORS = 6 \\ --env EXECUTOR_CORES = 2 \\ --env EXECUTOR_MEMORY = 768m \\ --volume $HOME /diffit/docker/files/python:/data \\ --volume $HOME /diffit/docker/files/schema:/tmp/schema \\ --volume $HOME /diffit/docker/files/data:/tmp/data \\ --hostname diffit-spark \\ --name diffit-spark \\ diffit:0.1.2a4-1 \\ row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right Note Container image execution is ephemeral. The --output switch ensures that the results persist on the file system via the mount provided by --volume $HOME/diffit/docker/files/data:/tmp/data Progress logs of the container execution can be monitored as follows: diffit on Spark pseudo-distributed logging make container-logs","title":"Running jobs on the Diffit Container Image"},{"location":"getting-started/","text":"Getting started Diffit can be used to validate PySpark transformation and aggregation logic at the Spark SQL DataFrame level. The diffit package can be installed by adding the following to your project's setup.cfg : setup.cfg as an extras require [ options.extras_require ] dev = diffit @ git+https://github.com/loum/diffit.git@<release_version> Note See Diffit releases for latest version. Editable installation can be achieved as follows: pip editable install pip install -e . [ dev ] Unit Testing The following example demonstrates how a complex aggregation with multiple Spark SQL DataFrame source data sets can be validated against an expected state: Unit testing a complex aggregation def test_aggregate ( diagnostic_recursive: DataFrame, diagnostic_static: DataFrame, cross_strategy_positions: DataFrame, portfolio_mapping: DataFrame, diagnostic_reporting: DataFrame ) : \"\"\"Diagnostic aggregate with static lookups. \"\"\" # Given a Diagnostic extended data set # diagnostic_recursive # and a Diagnostic static lookup data set # diagnostic_static # and the Cross Strategy Positions data set # cross_strategy_positions # and the Portfolio Mapping data set # portfolio_mapping # when I aggregate to produce a consumable data set for Diagnostic received = pgc.liquidity.reporting.diagnostic.aggregate ( diagnostic_recursive, diagnostic_static, cross_strategy_positions, portfolio_mapping ) # then the new DataFrame should align with the control Diagnostic consumed data set msg = 'Diagnostic consumed data set error' diffs: pyspark.sql.DataFrame = diffit.reporter.row_level ( left = diagnostic_reporting, right = received ) assert not [ list ( r ) for r in diffs.collect ()] , msg The diffit call of interest is diffit.reporter.row_level .Here a symmetric difference is performed against the left Spark SQL DataFrame named diagnostic_reporting and the received DataFrame produced by the aggregation output. For brievity, the assert simply validates that the diff variable does not contain content. This indicates that there is a problem and the test will fail. As diff is just a Spark SQL DataFrame itself, you can manipulate diff further in your analysis to identify the problem.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Diffit can be used to validate PySpark transformation and aggregation logic at the Spark SQL DataFrame level. The diffit package can be installed by adding the following to your project's setup.cfg : setup.cfg as an extras require [ options.extras_require ] dev = diffit @ git+https://github.com/loum/diffit.git@<release_version> Note See Diffit releases for latest version. Editable installation can be achieved as follows: pip editable install pip install -e . [ dev ]","title":"Getting started"},{"location":"getting-started/#unit-testing","text":"The following example demonstrates how a complex aggregation with multiple Spark SQL DataFrame source data sets can be validated against an expected state: Unit testing a complex aggregation def test_aggregate ( diagnostic_recursive: DataFrame, diagnostic_static: DataFrame, cross_strategy_positions: DataFrame, portfolio_mapping: DataFrame, diagnostic_reporting: DataFrame ) : \"\"\"Diagnostic aggregate with static lookups. \"\"\" # Given a Diagnostic extended data set # diagnostic_recursive # and a Diagnostic static lookup data set # diagnostic_static # and the Cross Strategy Positions data set # cross_strategy_positions # and the Portfolio Mapping data set # portfolio_mapping # when I aggregate to produce a consumable data set for Diagnostic received = pgc.liquidity.reporting.diagnostic.aggregate ( diagnostic_recursive, diagnostic_static, cross_strategy_positions, portfolio_mapping ) # then the new DataFrame should align with the control Diagnostic consumed data set msg = 'Diagnostic consumed data set error' diffs: pyspark.sql.DataFrame = diffit.reporter.row_level ( left = diagnostic_reporting, right = received ) assert not [ list ( r ) for r in diffs.collect ()] , msg The diffit call of interest is diffit.reporter.row_level .Here a symmetric difference is performed against the left Spark SQL DataFrame named diagnostic_reporting and the received DataFrame produced by the aggregation output. For brievity, the assert simply validates that the diff variable does not contain content. This indicates that there is a problem and the test will fail. As diff is just a Spark SQL DataFrame itself, you can manipulate diff further in your analysis to identify the problem.","title":"Unit Testing"},{"location":"utilities/","text":"diffit Tooling diffit provides a command line utility that you can use to invoke the Apache Spark-based symmetric differential engine. Usage venv/bin/diffit --help diffit help usage: diffit [ -h ] [ -m DRIVER_MEMORY ] { schema,row,analyse,columns,convert } ... Diff-it Data Diff tool positional arguments: { schema,row,analyse,columns,convert } schema Diffit schema control row DataFrame row-level diff analyse Diffit rows unique to source DataFrame columns Report only the columns that are different convert CSV to parquet optional arguments: -h, --help show this help message and exit -m DRIVER_MEMORY, --driver_memory DRIVER_MEMORY Set Spark driver memory ( default 2g )","title":"diffit Tooling"},{"location":"utilities/#diffit-tooling","text":"diffit provides a command line utility that you can use to invoke the Apache Spark-based symmetric differential engine.","title":"diffit Tooling"},{"location":"utilities/#usage","text":"venv/bin/diffit --help diffit help usage: diffit [ -h ] [ -m DRIVER_MEMORY ] { schema,row,analyse,columns,convert } ... Diff-it Data Diff tool positional arguments: { schema,row,analyse,columns,convert } schema Diffit schema control row DataFrame row-level diff analyse Diffit rows unique to source DataFrame columns Report only the columns that are different convert CSV to parquet optional arguments: -h, --help show this help message and exit -m DRIVER_MEMORY, --driver_memory DRIVER_MEMORY Set Spark driver memory ( default 2g )","title":"Usage"},{"location":"utilities/diffit/analyse/","text":"diffit analyse diffit analyse supports the concept of a unique constraint key to target differences at the row level. Usage venv/bin/diffit analyse --help usage: diffit analyse [ -h ] [ -R { left,right }] [ -D ] [ -C ] [ -H HITS ] [ -r RANGE ] [ -L LOWER ] [ -U UPPER ] [ -F ] { distinct,altered } key diffit_out positional arguments: { distinct,altered } Report analysis type key column that acts as a unique constraint diffit_out Path to Diffit output options: -h, --help show this help message and exit -R { left,right } , --diffit_ref { left,right } target data source reference -D, --descending Change output ordering to descending -C, --counts Only output counts -H HITS, --hits HITS Rows to display -r RANGE, --range RANGE Column to target for range filter -L LOWER, --lower LOWER Range filter lower bound ( inclusive ) -U UPPER, --upper UPPER Range filter upper bound ( inclusive ) -F, --force-range Force string-based range filter diffit analyse distinct diffit analyse distinct provides a way to report on rows from a Diffit extract that only appear in a specific target data source. Example: Analyse Rows Unique to Each Spark DataFrame This example reuses the Dummy.json schema from a previous example. Reset the Diffit extract venv/bin/diffit row tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right The key setting, col01, acts as the GROUP BY predicate venv/bin/diffit analyse distinct col01 /tmp/out Combined diffit analyse distinct output ### Analysing distinct rows from \"left\" source DataFrame +-----+-----+-----+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----+-----+----------+ +-----+-----+-----+----------+ ### Analysing distinct rows from \"right\" source DataFrame +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 9 | col02_val09 | col03_val09 | right | +-----+-----------+-----------+----------+ A Diffit extract can be limited with the --diffit_ref switch. For example, to only show distinct Diffit extract records from the right data source: Analysing distinct rows from right source Spark DataFrame venv/bin/diffit analyse --diffit_ref right distinct col01 /tmp/out Result ### Analysing distinct rows from \"right\" source DataFrame +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 9 | col02_val09 | col03_val09 | right | +-----+-----------+-----------+----------+ The default number of rows returned is 20 . This can be adjusted with the --hits switch: venv/bin/diffit analyse --diffit_ref right --hits 5 distinct col01 /tmp/out diffit analyse altered diffit analyse distinct provides a way to report on Diffit extract rows that appear in both target data sources and flagged as being different. Example Given a Diffit extract at /tmp/out that defines a schema column col01 that can act as the unique constraint: Reset the Diffit extract venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right diffit analyse altered command venv/bin/diffit analyse altered col01 /tmp/out Result +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 2 | col02_valXX | col03_val02 | right | | 8 | col02_val08 | col03_val08 | left | | 8 | col02_val08 | col03_valYY | right |","title":"diffit analyse"},{"location":"utilities/diffit/analyse/#diffit-analyse","text":"diffit analyse supports the concept of a unique constraint key to target differences at the row level.","title":"diffit analyse"},{"location":"utilities/diffit/analyse/#usage","text":"venv/bin/diffit analyse --help usage: diffit analyse [ -h ] [ -R { left,right }] [ -D ] [ -C ] [ -H HITS ] [ -r RANGE ] [ -L LOWER ] [ -U UPPER ] [ -F ] { distinct,altered } key diffit_out positional arguments: { distinct,altered } Report analysis type key column that acts as a unique constraint diffit_out Path to Diffit output options: -h, --help show this help message and exit -R { left,right } , --diffit_ref { left,right } target data source reference -D, --descending Change output ordering to descending -C, --counts Only output counts -H HITS, --hits HITS Rows to display -r RANGE, --range RANGE Column to target for range filter -L LOWER, --lower LOWER Range filter lower bound ( inclusive ) -U UPPER, --upper UPPER Range filter upper bound ( inclusive ) -F, --force-range Force string-based range filter","title":"Usage"},{"location":"utilities/diffit/analyse/#diffit-analyse-distinct","text":"diffit analyse distinct provides a way to report on rows from a Diffit extract that only appear in a specific target data source.","title":"diffit analyse distinct"},{"location":"utilities/diffit/analyse/#example-analyse-rows-unique-to-each-spark-dataframe","text":"This example reuses the Dummy.json schema from a previous example. Reset the Diffit extract venv/bin/diffit row tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right The key setting, col01, acts as the GROUP BY predicate venv/bin/diffit analyse distinct col01 /tmp/out Combined diffit analyse distinct output ### Analysing distinct rows from \"left\" source DataFrame +-----+-----+-----+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----+-----+----------+ +-----+-----+-----+----------+ ### Analysing distinct rows from \"right\" source DataFrame +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 9 | col02_val09 | col03_val09 | right | +-----+-----------+-----------+----------+ A Diffit extract can be limited with the --diffit_ref switch. For example, to only show distinct Diffit extract records from the right data source: Analysing distinct rows from right source Spark DataFrame venv/bin/diffit analyse --diffit_ref right distinct col01 /tmp/out Result ### Analysing distinct rows from \"right\" source DataFrame +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 9 | col02_val09 | col03_val09 | right | +-----+-----------+-----------+----------+ The default number of rows returned is 20 . This can be adjusted with the --hits switch: venv/bin/diffit analyse --diffit_ref right --hits 5 distinct col01 /tmp/out","title":"Example: Analyse Rows Unique to Each Spark DataFrame"},{"location":"utilities/diffit/analyse/#diffit-analyse-altered","text":"diffit analyse distinct provides a way to report on Diffit extract rows that appear in both target data sources and flagged as being different.","title":"diffit analyse altered"},{"location":"utilities/diffit/analyse/#example","text":"Given a Diffit extract at /tmp/out that defines a schema column col01 that can act as the unique constraint: Reset the Diffit extract venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right diffit analyse altered command venv/bin/diffit analyse altered col01 /tmp/out Result +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 2 | col02_valXX | col03_val02 | right | | 8 | col02_val08 | col03_val08 | left | | 8 | col02_val08 | col03_valYY | right |","title":"Example"},{"location":"utilities/diffit/columns/","text":"diffit columns Report on Spark DataFrame column/value pair differences. Diffit extracts can be large, based on the number of exceptions detected. diffit columns allows you to report on a specific key/value pairing for targeted analysis. Output is displayed as a JSON construct. Usage venv/bin/diffit columns --help usage: diffit columns [ -h ] key val diffit_out positional arguments: key column that acts as a unique constraint val unique constraint column value to filter against diffit_out Path to Diffit output options: -h, --help show this help message and exit Example Reset the Diffit extract venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right The Diffit extract at /tmp/out features: A schema column col01 acting as the unique constraint A key column value of 2 as a filter diffit columns filter for key:value pair col01:2 venv/bin/diffit columns col01 2 /tmp/out Result ### col01|2: [ { \"col02\" : \"col02_val02\" } ]","title":"diffit columns"},{"location":"utilities/diffit/columns/#diffit-columns","text":"Report on Spark DataFrame column/value pair differences. Diffit extracts can be large, based on the number of exceptions detected. diffit columns allows you to report on a specific key/value pairing for targeted analysis. Output is displayed as a JSON construct.","title":"diffit columns"},{"location":"utilities/diffit/columns/#usage","text":"venv/bin/diffit columns --help usage: diffit columns [ -h ] key val diffit_out positional arguments: key column that acts as a unique constraint val unique constraint column value to filter against diffit_out Path to Diffit output options: -h, --help show this help message and exit","title":"Usage"},{"location":"utilities/diffit/columns/#example","text":"Reset the Diffit extract venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right The Diffit extract at /tmp/out features: A schema column col01 acting as the unique constraint A key column value of 2 as a filter diffit columns filter for key:value pair col01:2 venv/bin/diffit columns col01 2 /tmp/out Result ### col01|2: [ { \"col02\" : \"col02_val02\" } ]","title":"Example"},{"location":"utilities/diffit/convert/","text":"diffit convert Convert as CSV data source with schema file to Spark Parquet to with a given compression (default snappy ) Usage diffit convert help venv/bin/diffit convert --help usage: diffit convert [ -h ] [ -z { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd }] schema data_source output positional arguments: schema CSV schema to convert data_source CSV source location output Write parquet to path options: -h, --help show this help message and exit -z { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd } , --compression { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd } Compression type","title":"diffit convert"},{"location":"utilities/diffit/convert/#diffit-convert","text":"Convert as CSV data source with schema file to Spark Parquet to with a given compression (default snappy )","title":"diffit convert"},{"location":"utilities/diffit/convert/#usage","text":"diffit convert help venv/bin/diffit convert --help usage: diffit convert [ -h ] [ -z { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd }] schema data_source output positional arguments: schema CSV schema to convert data_source CSV source location output Write parquet to path options: -h, --help show this help message and exit -z { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd } , --compression { brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd } Compression type","title":"Usage"},{"location":"utilities/diffit/row/","text":"diffit row diffit row reporter acts on two data sources with the same schema. These are denoted as left and right . Differences are reported at the row level. Both CSV and Spark Parquet data sources are supported. Another way to think about the diffit row reporter is that identical rows from the left and right data sources are suppressed from the output. diffit row will produce a Diffit extract that is a Spark DataFrame in Spark Parquet format. The Diffit extract can then be further analysed using other diffit subcommands see diffit analyse or with any other tooling that supports parquet. A key characteristic of the Diffit extract is that it features a new column diffit_ref . This denotes the source reference that has caused the row level exception. Typically, this value will be either left or right . Usage diffit row help venv/bin/diffit row --help usage: diffit row [ -h ] [ -o OUTPUT ] [ -d [ DROP ... ]] [ -r RANGE ] [ -L LOWER ] [ -U UPPER ] [ -F ] { csv,parquet } ... options: -h, --help show this help message and exit -o OUTPUT, --output OUTPUT Write results to path -d [ DROP ... ] , --drop [ DROP ... ] Drop column from diffit engine -r RANGE, --range RANGE Column to target for range filter -L LOWER, --lower LOWER Range filter lower bound ( inclusive ) -U UPPER, --upper UPPER Range filter upper bound ( inclusive ) -F, --force-range Force string-based range filter sub-commands: { csv,parquet } csv CSV row-level parquet Parquet row-level CSV Data Sources CSV files will require a schema definition. This needs to be provided as a JSON construct. diffit row csv help venv/bin/diffit row csv --help usage: diffit row csv [ -h ] [ -s CSV_SEPARATOR ] [ -E ] schema left_data_source right_data_source positional arguments: schema Path to CSV schema in JSON format left_data_source \"Left\" CSV source location right_data_source \"Right\" CSV source location options: -h, --help show this help message and exit -s CSV_SEPARATOR, --csv-separator CSV_SEPARATOR CSV separator -E, --csv-header CSV contains header Example: CSV Data Sources Save the following sample JSON schema definition to /tmp/Dummy.json : Example CSV JSON Schema { \"type\" : \"struct\" , \"fields\" : [ { \"name\" : \"col01\" , \"type\" : \"integer\" , \"nullable\" : true, \"metadata\" : {} } , { \"name\" : \"col02\" , \"type\" : \"string\" , \"nullable\" : true, \"metadata\" : {} } ] } Next, run the CSV row comparitor: diffit row csv command venv/bin/diffit row csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right diffit row csv example output +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 8 | col02_val08 | col03_val08 | left | | 9 | col02_val09 | col03_val09 | right | | 2 | col02_valXX | col03_val02 | right | | 8 | col02_val08 | col03_valYY | right | +-----+-----------+-----------+----------+ Parquet Data Sources Take advantage of the nice features of Spark Parquet. diffit row parquet help venv/bin/diffit row parquet --help usage: diffit row parquet [ -h ] left_data_source right_data_source positional arguments: left_data_source \"Left\" Parquet source location right_data_source \"Right\" Parquet source location options: -h, --help show this help message and exit Report on Subset of Spark DataFrame Columns diffit can be run on a subset of DataFrame columns. This can limit the symmetric difference checker to a reduced number of colums for more targeted, efficient processing. To remove one or more unwanted Spark DataFrame columns use the drop switch. For example, to drop col02 from the local test sample: Example: CSV Data Sources with Column Filtering diffit row csv dropping a column from the symmetric differential engine venv/bin/diffit row --drop col02 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+-----------+----------+ | col01 | col03 | diffit_ref | +-----+-----------+----------+ | 8 | col03_val08 | left | | 8 | col03_valYY | right | | 9 | col03_val09 | right | +-----+-----------+----------+ Multiple columns can be added to the drop switch separated by spaces. For example: ... --drop col01 --drop col02 ... --drop <col_n> Dropping multiple columns from symmetric differential engine venv/bin/diffit row --drop col02 --drop col03 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+----------+ | col01 | diffit_ref | +-----+----------+ | 9 | right | +-----+----------+ Column Value Range Filtering Note Only pyspark.sql.types.IntegerType is currently supported. Filtering can be useful to limit diffit to a subset of the original Spark SQL DataFrame. For example, we can limit the test data sources under docker/files/data/left to remove col01 values 3 and above as follows: Example: CSV Data Sources with Output Reduced through Range Filtering Column range filtering venv/bin/diffit row --range col01 --lower 1 --upper 2 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 2 | col02_valXX | col03_val02 | right | +-----+-----------+-----------+----------","title":"diffit row"},{"location":"utilities/diffit/row/#diffit-row","text":"diffit row reporter acts on two data sources with the same schema. These are denoted as left and right . Differences are reported at the row level. Both CSV and Spark Parquet data sources are supported. Another way to think about the diffit row reporter is that identical rows from the left and right data sources are suppressed from the output. diffit row will produce a Diffit extract that is a Spark DataFrame in Spark Parquet format. The Diffit extract can then be further analysed using other diffit subcommands see diffit analyse or with any other tooling that supports parquet. A key characteristic of the Diffit extract is that it features a new column diffit_ref . This denotes the source reference that has caused the row level exception. Typically, this value will be either left or right .","title":"diffit row"},{"location":"utilities/diffit/row/#usage","text":"diffit row help venv/bin/diffit row --help usage: diffit row [ -h ] [ -o OUTPUT ] [ -d [ DROP ... ]] [ -r RANGE ] [ -L LOWER ] [ -U UPPER ] [ -F ] { csv,parquet } ... options: -h, --help show this help message and exit -o OUTPUT, --output OUTPUT Write results to path -d [ DROP ... ] , --drop [ DROP ... ] Drop column from diffit engine -r RANGE, --range RANGE Column to target for range filter -L LOWER, --lower LOWER Range filter lower bound ( inclusive ) -U UPPER, --upper UPPER Range filter upper bound ( inclusive ) -F, --force-range Force string-based range filter sub-commands: { csv,parquet } csv CSV row-level parquet Parquet row-level","title":"Usage"},{"location":"utilities/diffit/row/#csv-data-sources","text":"CSV files will require a schema definition. This needs to be provided as a JSON construct. diffit row csv help venv/bin/diffit row csv --help usage: diffit row csv [ -h ] [ -s CSV_SEPARATOR ] [ -E ] schema left_data_source right_data_source positional arguments: schema Path to CSV schema in JSON format left_data_source \"Left\" CSV source location right_data_source \"Right\" CSV source location options: -h, --help show this help message and exit -s CSV_SEPARATOR, --csv-separator CSV_SEPARATOR CSV separator -E, --csv-header CSV contains header","title":"CSV Data Sources"},{"location":"utilities/diffit/row/#example-csv-data-sources","text":"Save the following sample JSON schema definition to /tmp/Dummy.json : Example CSV JSON Schema { \"type\" : \"struct\" , \"fields\" : [ { \"name\" : \"col01\" , \"type\" : \"integer\" , \"nullable\" : true, \"metadata\" : {} } , { \"name\" : \"col02\" , \"type\" : \"string\" , \"nullable\" : true, \"metadata\" : {} } ] } Next, run the CSV row comparitor: diffit row csv command venv/bin/diffit row csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right diffit row csv example output +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 8 | col02_val08 | col03_val08 | left | | 9 | col02_val09 | col03_val09 | right | | 2 | col02_valXX | col03_val02 | right | | 8 | col02_val08 | col03_valYY | right | +-----+-----------+-----------+----------+","title":"Example: CSV Data Sources"},{"location":"utilities/diffit/row/#parquet-data-sources","text":"Take advantage of the nice features of Spark Parquet. diffit row parquet help venv/bin/diffit row parquet --help usage: diffit row parquet [ -h ] left_data_source right_data_source positional arguments: left_data_source \"Left\" Parquet source location right_data_source \"Right\" Parquet source location options: -h, --help show this help message and exit","title":"Parquet Data Sources"},{"location":"utilities/diffit/row/#report-on-subset-of-spark-dataframe-columns","text":"diffit can be run on a subset of DataFrame columns. This can limit the symmetric difference checker to a reduced number of colums for more targeted, efficient processing. To remove one or more unwanted Spark DataFrame columns use the drop switch. For example, to drop col02 from the local test sample:","title":"Report on Subset of Spark DataFrame Columns"},{"location":"utilities/diffit/row/#example-csv-data-sources-with-column-filtering","text":"diffit row csv dropping a column from the symmetric differential engine venv/bin/diffit row --drop col02 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+-----------+----------+ | col01 | col03 | diffit_ref | +-----+-----------+----------+ | 8 | col03_val08 | left | | 8 | col03_valYY | right | | 9 | col03_val09 | right | +-----+-----------+----------+ Multiple columns can be added to the drop switch separated by spaces. For example: ... --drop col01 --drop col02 ... --drop <col_n> Dropping multiple columns from symmetric differential engine venv/bin/diffit row --drop col02 --drop col03 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+----------+ | col01 | diffit_ref | +-----+----------+ | 9 | right | +-----+----------+","title":"Example: CSV Data Sources with Column Filtering"},{"location":"utilities/diffit/row/#column-value-range-filtering","text":"Note Only pyspark.sql.types.IntegerType is currently supported. Filtering can be useful to limit diffit to a subset of the original Spark SQL DataFrame. For example, we can limit the test data sources under docker/files/data/left to remove col01 values 3 and above as follows:","title":"Column Value Range Filtering"},{"location":"utilities/diffit/row/#example-csv-data-sources-with-output-reduced-through-range-filtering","text":"Column range filtering venv/bin/diffit row --range col01 --lower 1 --upper 2 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right Result +-----+-----------+-----------+----------+ | col01 | col02 | col03 | diffit_ref | +-----+-----------+-----------+----------+ | 2 | col02_val02 | col03_val02 | left | | 2 | col02_valXX | col03_val02 | right | +-----+-----------+-----------+----------","title":"Example: CSV Data Sources with Output Reduced through Range Filtering"}]}