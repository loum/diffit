{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Spark SQL DataFrame Symmetric Difference Engine","text":"<p>The concept of symmetric difference in mathematics defines the set of elements that do not appear in either of the sets. It is leveraged within the RDBMS community as a strategy for identifying differences across two tables. <code>diffit</code> extends this capability to Apache Spark SQL DataFrames.</p> <p>Diffit was created in response to challenges around validating massive data sets that use big data file formats. However, Diffit is also flexible enough to be used within your unit test framework.</p>"},{"location":"containerisation/","title":"Scaling out Diffit","text":"<p>Diffit can be scaled out to perform symmetric difference analysis across massive data sets. This is a natural extension as Diffit is built onto of the Apache Spark compute framework</p>"},{"location":"containerisation/#diffit-container-image","title":"Diffit Container Image","text":"<p>Note</p> <p>Diffit image container to be provided via Docker Hub shortly.</p>"},{"location":"containerisation/#manual-build-of-the-diffit-container-image","title":"Manual Build of the Diffit Container Image","text":"<p>The Diffit image container is built on top of this Spark base image that runs over YARN on Pseudo Distributed Hadoop. The goal here is to establish a pathway into a larger Spark cluster.</p> <p>The container image build process is started with: Starting the Diffit container image build process<pre><code>make diffit-image-build\n</code></pre></p> <p>On successful completion, you can search for the newly created container image with: Container image search<pre><code>make image-search\n</code></pre></p>"},{"location":"containerisation/#running-jobs-on-the-diffit-container-image","title":"Running jobs on the Diffit Container Image","text":"<p>Refer to the Diffit <code>Makefile</code> for the targets used in this section.</p> <p>Note</p> <p>All Makester Docker subsystem targets can be listed with: <pre><code>make docker-help`\n</code></pre></p> <p>Here is how the <code>diffit row csv</code> example would be run in the container image: Running diffit compute on container image<pre><code>CMD=\"row csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\" make container-run\n</code></pre></p> <p>This is the actual <code>docker</code> command: Diffit container run make target<pre><code>make container-run\n</code></pre></p> <pre><code>/usr/bin/docker run --rm -d\\\n--platform linux/amd64\\\n--publish 8032:8032\\\n--publish 7077:7077\\\n--publish 8080:8080\\\n--publish 8088:8088\\\n--publish 8042:8042\\\n--publish 18080:18080\\\n--env CORE_SITE__HADOOP_TMP_DIR=/tmp\\\n--env HDFS_SITE__DFS_REPLICATION=1\\\n--env YARN_SITE__YARN_NODEMANAGER_RESOURCE_DETECT_HARDWARE_CAPABILITIES=true\\\n--env YARN_SITE__YARN_LOG_AGGREGATION_ENABLE=true\\\n--env OUTPUT_PATH=file:///tmp/data/out\\\n--env NUM_EXECUTORS=6\\\n--env EXECUTOR_CORES=2\\\n--env EXECUTOR_MEMORY=768m\\\n--volume $HOME/diffit/docker/files/python:/data\\\n--volume $HOME/diffit/docker/files/schema:/tmp/schema\\\n--volume $HOME/diffit/docker/files/data:/tmp/data\\\n--hostname diffit-spark\\\n--name diffit-spark\\\ndiffit:0.1.2a4-1\\\nrow csv --output file:///tmp/data/out --csv-separator ';' /tmp/schema/Dummy.json file:///tmp/data/left file:///tmp/data/right\n</code></pre> <p>Note</p> <p>Container image execution is ephemeral. The <code>--output</code> switch ensures that the results persist on the file system via the mount provided by <code>--volume $HOME/diffit/docker/files/data:/tmp/data</code></p> <p>Progress logs of the container execution can be monitored as follows: diffit on Spark pseudo-distributed logging<pre><code>make container-logs\n</code></pre></p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Diffit can be used to validate PySpark transformation and aggregation logic at the Spark SQL DataFrame level. The <code>diffit</code> package can be installed by adding the following to your project's <code>setup.cfg</code>: setup.cfg as an extras require<pre><code>[options.extras_require]\ndev =\ndiffit @ git+https://github.com/loum/diffit.git@&lt;release_version&gt;\n</code></pre></p> <p>Note</p> <p>See Diffit releases for latest version.</p> <p>Editable installation can be achieved as follows: pip editable install<pre><code>pip install -e .[dev]\n</code></pre></p>"},{"location":"getting-started/#unit-testing","title":"Unit Testing","text":"<p>The following example demonstrates how a complex aggregation with multiple Spark SQL DataFrame source data sets can be validated against an expected state: Unit testing a complex aggregation<pre><code>def test_aggregate(diagnostic_recursive: DataFrame,\n                   diagnostic_static: DataFrame,\n                   cross_strategy_positions: DataFrame,\n                   portfolio_mapping: DataFrame,\n                   diagnostic_reporting: DataFrame):\n    \"\"\"Diagnostic aggregate with static lookups.\n    \"\"\"\n# Given a Diagnostic extended data set\n# diagnostic_recursive\n# and a Diagnostic static lookup data set\n# diagnostic_static\n# and the Cross Strategy Positions data set\n# cross_strategy_positions\n# and the Portfolio Mapping data set\n# portfolio_mapping\n# when I aggregate to produce a consumable data set for Diagnostic\nreceived = pgc.liquidity.reporting.diagnostic.aggregate(diagnostic_recursive,\n                                                            diagnostic_static,\n                                                            cross_strategy_positions,\n                                                            portfolio_mapping)\n# then the new DataFrame should align with the control Diagnostic consumed data set\nmsg = 'Diagnostic consumed data set error'\ndiffs: pyspark.sql.DataFrame = diffit.reporter.row_level(left=diagnostic_reporting, right=received)\nassert not [list(r) for r in diffs.collect()], msg\n</code></pre></p> <p>The <code>diffit</code> call of interest is <code>diffit.reporter.row_level</code>.Here a symmetric difference is performed against the <code>left</code> Spark SQL DataFrame named <code>diagnostic_reporting</code> and the <code>received</code> DataFrame produced by the aggregation output.</p> <p>For brievity, the <code>assert</code> simply validates that the <code>diff</code> variable does not contain content. This indicates that there is a problem and the test will fail. As <code>diff</code> is just a Spark SQL DataFrame itself, you can manipulate <code>diff</code> further in your analysis to identify the problem.</p>"},{"location":"utilities/","title":"diffit Tooling","text":"<p><code>diffit</code> provides a command line utility that you can use to invoke the Apache Spark-based symmetric differential engine.</p>"},{"location":"utilities/#usage","title":"Usage","text":"<pre><code>venv/bin/diffit --help\n</code></pre> diffit help<pre><code>usage: diffit [-h] [-m DRIVER_MEMORY] {schema,row,analyse,columns,convert} ...\n\nDiff-it Data Diff tool\n\npositional arguments:\n  {schema,row,analyse,columns,convert}\nschema              Diffit schema control\n    row                 DataFrame row-level diff\n    analyse             Diffit rows unique to source DataFrame\n    columns             Report only the columns that are different\n    convert             CSV to parquet\n\noptional arguments:\n  -h, --help            show this help message and exit\n-m DRIVER_MEMORY, --driver_memory DRIVER_MEMORY\n                        Set Spark driver memory (default 2g)\n</code></pre>"},{"location":"utilities/diffit/analyse/","title":"diffit analyse","text":"<p><code>diffit analyse</code> supports the concept of a unique constraint key to target differences at the row level.</p>"},{"location":"utilities/diffit/analyse/#usage","title":"Usage","text":"<pre><code>venv/bin/diffit analyse --help\n</code></pre> <pre><code>usage: diffit analyse [-h] [-R {left,right}] [-D] [-C] [-H HITS] [-r RANGE] [-L LOWER] [-U UPPER] [-F]\n{distinct,altered} key diffit_out\n\npositional arguments:\n  {distinct,altered}    Report analysis type\nkey                   column that acts as a unique constraint\n  diffit_out            Path to Diffit output\n\noptions:\n  -h, --help            show this help message and exit\n-R {left,right}, --diffit_ref {left,right}\ntarget data source reference\n  -D, --descending      Change output ordering to descending\n  -C, --counts          Only output counts\n  -H HITS, --hits HITS  Rows to display\n  -r RANGE, --range RANGE\n                        Column to target for range filter\n  -L LOWER, --lower LOWER\n                        Range filter lower bound (inclusive)\n-U UPPER, --upper UPPER\n                        Range filter upper bound (inclusive)\n-F, --force-range     Force string-based range filter\n</code></pre>"},{"location":"utilities/diffit/analyse/#diffit-analyse-distinct","title":"<code>diffit analyse distinct</code>","text":"<p><code>diffit analyse distinct</code> provides a way to report on rows from a Diffit extract that only appear in a specific target data source.</p>"},{"location":"utilities/diffit/analyse/#example-analyse-rows-unique-to-each-spark-dataframe","title":"Example: Analyse Rows Unique to Each Spark DataFrame","text":"<p>This example reuses the Dummy.json schema from a previous example. Reset the Diffit extract<pre><code>venv/bin/diffit row tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre></p> The key setting, col01, acts as the GROUP BY predicate<pre><code>venv/bin/diffit analyse distinct col01 /tmp/out\n</code></pre> Combined diffit analyse distinct output<pre><code>### Analysing distinct rows from \"left\" source DataFrame\n+-----+-----+-----+----------+\n|col01|col02|col03|diffit_ref|\n+-----+-----+-----+----------+\n+-----+-----+-----+----------+\n\n### Analysing distinct rows from \"right\" source DataFrame\n+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|9    |col02_val09|col03_val09|right     |\n+-----+-----------+-----------+----------+\n</code></pre> <p>A Diffit extract can be limited with the <code>--diffit_ref</code> switch. For example, to only show distinct Diffit extract records from the <code>right</code> data source: Analysing distinct rows from right source Spark DataFrame<pre><code>venv/bin/diffit analyse --diffit_ref right distinct col01 /tmp/out\n</code></pre></p> Result<pre><code>### Analysing distinct rows from \"right\" source DataFrame\n+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|9    |col02_val09|col03_val09|right     |\n+-----+-----------+-----------+----------+\n</code></pre> <p>The default number of rows returned is <code>20</code>. This can be adjusted with the <code>--hits</code> switch: <pre><code>venv/bin/diffit analyse --diffit_ref right --hits 5 distinct col01 /tmp/out\n</code></pre></p>"},{"location":"utilities/diffit/analyse/#diffit-analyse-altered","title":"<code>diffit analyse altered</code>","text":"<p><code>diffit analyse distinct</code> provides a way to report on Diffit extract rows that appear in both target data sources and flagged as being different.</p>"},{"location":"utilities/diffit/analyse/#example","title":"Example","text":"<p>Given a Diffit extract at <code>/tmp/out</code> that defines a schema column <code>col01</code> that can act as the unique constraint:</p> Reset the Diffit extract<pre><code>venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre> diffit analyse altered command<pre><code>venv/bin/diffit analyse altered col01 /tmp/out\n</code></pre> Result<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n|8    |col02_val08|col03_val08|left      |\n|8    |col02_val08|col03_valYY|right     |\n</code></pre>"},{"location":"utilities/diffit/columns/","title":"diffit columns","text":"<p>Report on Spark DataFrame column/value pair differences.</p> <p>Diffit extracts can be large, based on the number of exceptions detected. <code>diffit columns</code> allows you to report on a specific key/value pairing for targeted analysis.</p> <p>Output is displayed as a JSON construct.</p>"},{"location":"utilities/diffit/columns/#usage","title":"Usage","text":"<pre><code>venv/bin/diffit columns --help\n</code></pre> <pre><code>usage: diffit columns [-h] key val diffit_out\n\npositional arguments:\n  key         column that acts as a unique constraint\n  val         unique constraint column value to filter against\n  diffit_out  Path to Diffit output\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"utilities/diffit/columns/#example","title":"Example","text":"Reset the Diffit extract<pre><code>venv/bin/diffit row --output /tmp/out csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre> <p>The Diffit extract at <code>/tmp/out</code> features:</p> <ul> <li>A schema column <code>col01</code> acting as the unique constraint</li> <li>A key column value of <code>2</code> as a filter</li> </ul> diffit columns filter for key:value pair col01:2<pre><code>venv/bin/diffit columns col01 2 /tmp/out\n</code></pre> Result<pre><code>### col01|2: [\n{\n\"col02\": \"col02_val02\"\n}\n]\n</code></pre>"},{"location":"utilities/diffit/convert/","title":"diffit convert","text":"<p>Convert as CSV data source with schema file to Spark Parquet to with a given compression (default <code>snappy</code>)</p>"},{"location":"utilities/diffit/convert/#usage","title":"Usage","text":"diffit convert help<pre><code>venv/bin/diffit convert --help\n</code></pre> <pre><code>usage: diffit convert [-h] [-z {brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd}] schema data_source output\n\npositional arguments:\n  schema                CSV schema to convert\n  data_source           CSV source location\n  output                Write parquet to path\n\noptions:\n  -h, --help            show this help message and exit\n-z {brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd}, --compression {brotli,uncompressed,lz4,gzip,lzo,snappy,none,zstd}\nCompression type\n</code></pre>"},{"location":"utilities/diffit/row/","title":"diffit row","text":"<p><code>diffit row</code> reporter acts on two data sources with the same schema. These are denoted as <code>left</code> and <code>right</code>. Differences are reported at the row level. Both CSV and Spark Parquet data sources are supported. </p> <p>Another way to think about the <code>diffit row</code> reporter is that identical rows from the <code>left</code> and <code>right</code> data sources are suppressed from the output.</p> <p><code>diffit row</code> will produce a Diffit extract that is a Spark DataFrame in Spark Parquet format. The Diffit extract can then be further analysed using other <code>diffit</code> subcommands see <code>diffit analyse</code> or with any other tooling that supports parquet.</p> <p>A key characteristic of the Diffit extract is that it features a new column <code>diffit_ref</code>. This denotes the source reference that has caused the row level exception. Typically, this value will be either <code>left</code> or <code>right</code>.</p>"},{"location":"utilities/diffit/row/#usage","title":"Usage","text":"diffit row help<pre><code>venv/bin/diffit row --help\n</code></pre> <pre><code>usage: diffit row [-h] [-o OUTPUT] [-d [DROP ...]] [-r RANGE] [-L LOWER] [-U UPPER] [-F] {csv,parquet} ...\n\noptions:\n  -h, --help            show this help message and exit\n-o OUTPUT, --output OUTPUT\n                        Write results to path\n  -d [DROP ...], --drop [DROP ...]\nDrop column from diffit engine\n  -r RANGE, --range RANGE\n                        Column to target for range filter\n  -L LOWER, --lower LOWER\n                        Range filter lower bound (inclusive)\n-U UPPER, --upper UPPER\n                        Range filter upper bound (inclusive)\n-F, --force-range     Force string-based range filter\n\nsub-commands:\n  {csv,parquet}\ncsv                 CSV row-level\n    parquet             Parquet row-level\n</code></pre>"},{"location":"utilities/diffit/row/#csv-data-sources","title":"CSV Data Sources","text":"<p>CSV files will require a schema definition. This needs to be provided as a JSON construct.</p> diffit row csv help<pre><code>venv/bin/diffit row csv --help\n</code></pre> <pre><code>usage: diffit row csv [-h] [-s CSV_SEPARATOR] [-E] schema left_data_source right_data_source\n\npositional arguments:\n  schema                Path to CSV schema in JSON format\n  left_data_source      \"Left\" CSV source location\n  right_data_source     \"Right\" CSV source location\n\noptions:\n  -h, --help            show this help message and exit\n-s CSV_SEPARATOR, --csv-separator CSV_SEPARATOR\n                        CSV separator\n  -E, --csv-header      CSV contains header\n</code></pre>"},{"location":"utilities/diffit/row/#example-csv-data-sources","title":"Example: CSV Data Sources","text":"<p>Save the following sample JSON schema definition to <code>/tmp/Dummy.json</code>: Example CSV JSON Schema<pre><code>{\n\"type\": \"struct\",\n    \"fields\": [\n{\n\"name\" : \"col01\",\n            \"type\" : \"integer\",\n            \"nullable\": true,\n            \"metadata\": {}\n},\n        {\n\"name\": \"col02\",\n            \"type\": \"string\",\n            \"nullable\": true,\n            \"metadata\": {}\n}\n]\n}\n</code></pre></p> <p>Next, run the CSV row comparitor: diffit row csv command<pre><code>venv/bin/diffit row csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre></p> diffit row csv example output<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|8    |col02_val08|col03_val08|left      |\n|9    |col02_val09|col03_val09|right     |\n|2    |col02_valXX|col03_val02|right     |\n|8    |col02_val08|col03_valYY|right     |\n+-----+-----------+-----------+----------+\n</code></pre>"},{"location":"utilities/diffit/row/#parquet-data-sources","title":"Parquet Data Sources","text":"<p>Take advantage of the nice features of Spark Parquet. diffit row parquet help<pre><code>venv/bin/diffit row parquet --help\n</code></pre></p> <pre><code>usage: diffit row parquet [-h] left_data_source right_data_source\n\npositional arguments:\n  left_data_source   \"Left\" Parquet source location\n  right_data_source  \"Right\" Parquet source location\n\noptions:\n  -h, --help         show this help message and exit\n</code></pre>"},{"location":"utilities/diffit/row/#report-on-subset-of-spark-dataframe-columns","title":"Report on Subset of Spark DataFrame Columns","text":"<p><code>diffit</code> can be run on a subset of DataFrame columns. This can limit the symmetric difference checker to a reduced number of colums for more targeted, efficient processing.</p> <p>To remove one or more unwanted Spark DataFrame columns use the <code>drop</code> switch. For example, to drop <code>col02</code> from the local test sample:</p>"},{"location":"utilities/diffit/row/#example-csv-data-sources-with-column-filtering","title":"Example: CSV Data Sources with Column Filtering","text":"diffit row csv dropping a column from the symmetric differential engine<pre><code>venv/bin/diffit row --drop col02 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre> Result<pre><code>+-----+-----------+----------+\n|col01|col03      |diffit_ref|\n+-----+-----------+----------+\n|8    |col03_val08|left      |\n|8    |col03_valYY|right     |\n|9    |col03_val09|right     |\n+-----+-----------+----------+\n</code></pre> <p>Multiple columns can be added to the <code>drop</code> switch separated by spaces. For example: <pre><code>... --drop col01 --drop col02 ... --drop &lt;col_n&gt; </code></pre></p> Dropping multiple columns from symmetric differential engine<pre><code>venv/bin/diffit row --drop col02 --drop col03 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre> Result<pre><code>+-----+----------+\n|col01|diffit_ref|\n+-----+----------+\n|9    |right     |\n+-----+----------+\n</code></pre>"},{"location":"utilities/diffit/row/#column-value-range-filtering","title":"Column Value Range Filtering","text":"<p>Note</p> <p>Only <code>pyspark.sql.types.IntegerType</code> is currently supported.</p> <p>Filtering can be useful to limit <code>diffit</code> to a subset of the original Spark SQL DataFrame. For example, we can limit the test data sources under <code>docker/files/data/left</code> to remove <code>col01</code> values <code>3</code> and above as follows:</p>"},{"location":"utilities/diffit/row/#example-csv-data-sources-with-output-reduced-through-range-filtering","title":"Example: CSV Data Sources with Output Reduced through Range Filtering","text":"Column range filtering<pre><code>venv/bin/diffit row --range col01 --lower 1 --upper 2 csv --csv-separator ';' /tmp/Dummy.json docker/files/data/left docker/files/data/right\n</code></pre> Result<pre><code>+-----+-----------+-----------+----------+\n|col01|col02      |col03      |diffit_ref|\n+-----+-----------+-----------+----------+\n|2    |col02_val02|col03_val02|left      |\n|2    |col02_valXX|col03_val02|right     |\n+-----+-----------+-----------+----------\n</code></pre>"}]}